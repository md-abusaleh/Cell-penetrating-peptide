{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMp6Sgr5UswWN66IdruDZKm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"P-hn2hl8k9W_"},"outputs":[],"source":["!pip install optuna"]},{"cell_type":"code","source":["!pip install catboost"],"metadata":{"id":"i4g_FQt8m_fn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install scikeras"],"metadata":{"id":"nzcpGB_Am_hv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import libraries\n","import optuna\n","import pandas as pd\n","from sklearn.model_selection import StratifiedKFold, cross_val_score\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","from catboost import CatBoostClassifier\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from scikeras.wrappers import KerasClassifier\n","# Load datasets\n","main_p = pd.read_csv(\"/content/drive/MyDrive/NEW_WORK/work_2/5_CTDT/CTDT_main_positive_features.csv\")\n","main_n = pd.read_csv(\"/content/drive/MyDrive/NEW_WORK/work_2/5_CTDT/CTDT_main_negative_features.csv\")\n","validation_p = pd.read_csv(\"/content/drive/MyDrive/NEW_WORK/work_2/5_CTDT/CTDT_validation_positive_features.csv\")\n","validation_n = pd.read_csv(\"/content/drive/MyDrive/NEW_WORK/work_2/5_CTDT/CTDT_validation_negative_features.csv\")\n","\n"],"metadata":{"id":"FZrMaY_im_kW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import optuna\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import StratifiedKFold, cross_val_score\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","# Combine positive and negative samples\n","\n","\n","X_train = pd.concat([main_p, main_n])\n","y_train = np.concatenate([np.ones(len(main_p)), np.zeros(len(main_n))])\n","# Define a cross-validation accuracy function\n","def cross_val_accuracy(model, X_train, y_train):\n","    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","    accuracies = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n","    return accuracies.mean()\n","\n","# Define the objective function for grouped models\n","def objective(trial, model_name, X_train, y_train):\n","    if model_name == \"SVM\":\n","        kernel = trial.suggest_categorical('kernel', ['linear', 'rbf'])\n","        C = trial.suggest_loguniform('C', 1e-5, 1e5)\n","        gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n","        model = SVC(kernel=kernel, C=C, gamma=gamma)\n","\n","    elif model_name == \"Decision Tree\":\n","        max_depth = trial.suggest_int('max_depth', 3, 30)\n","        min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n","        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5)\n","        criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n","        model = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split,\n","                                       min_samples_leaf=min_samples_leaf, criterion=criterion)\n","\n","    elif model_name == \"Random Forest\":\n","        n_estimators = trial.suggest_int('n_estimators', 100, 300)\n","        max_depth = trial.suggest_int('max_depth', 3, 30)\n","        max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n","        min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n","        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,\n","                                        max_features=max_features, min_samples_split=min_samples_split)\n","\n","\n","    # Calculate cross-validation accuracy\n","    score = cross_val_accuracy(model, X_train, y_train)\n","    return score\n","\n","# Grouped models for optimization\n","grouped_models = [\"SVM\", \"Decision Tree\", \"Random Forest\"]\n","\n","# Store results\n","grouped_best_params = {}\n","grouped_best_accuracies = []\n","\n","for model_name in grouped_models:\n","    print(f\"Optimizing {model_name}...\")\n","    study = optuna.create_study(direction='maximize')\n","    study.optimize(lambda trial: objective(trial, model_name, X_train, y_train), n_trials=50)\n","    grouped_best_params[model_name] = study.best_params\n","    grouped_best_accuracies.append(study.best_value)\n","\n","# Save results\n","grouped_results = pd.DataFrame({\n","    'Model': grouped_models,\n","    'Best Accuracy': grouped_best_accuracies,\n","    'Best Parameters': [grouped_best_params[model] for model in grouped_models]\n","})\n","print(grouped_results)\n"],"metadata":{"id":"cxEx-hHwm_m2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Gradient Boosting\n","\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","def objective_gradient_boosting(trial):\n","    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n","    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n","    max_depth = trial.suggest_int('max_depth', 3, 20)\n","    subsample = trial.suggest_uniform('subsample', 0.5, 1.0)\n","\n","    model = GradientBoostingClassifier(\n","        n_estimators=n_estimators,\n","        learning_rate=learning_rate,\n","        max_depth=max_depth,\n","        subsample=subsample\n","    )\n","\n","    score = cross_val_accuracy(model, X_train, y_train)\n","    return score\n","\n","# Run Optuna for Gradient Boosting\n","study_gradient_boosting = optuna.create_study(direction='maximize')\n","study_gradient_boosting.optimize(objective_gradient_boosting, n_trials=50)\n","\n","# Print and save results\n","print(\"Gradient Boosting Results:\")\n","print(\"Best Parameters:\", study_gradient_boosting.best_params)\n","print(\"Best Accuracy:\", study_gradient_boosting.best_value)\n","\n","# Save results\n","gradient_boosting_results = pd.DataFrame([{\n","    'Model': 'Gradient Boosting',\n","    'Best Accuracy': study_gradient_boosting.best_value,\n","    'Best Parameters': study_gradient_boosting.best_params\n","}])\n"],"metadata":{"id":"jhB4Ux-snGVW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#XGBoost\n","\n","from xgboost import XGBClassifier\n","\n","def objective_xgboost(trial):\n","    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n","    max_depth = trial.suggest_int('max_depth', 3, 20)\n","    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n","    colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.5, 1.0)\n","\n","    model = XGBClassifier(\n","        n_estimators=n_estimators,\n","        max_depth=max_depth,\n","        learning_rate=learning_rate,\n","        colsample_bytree=colsample_bytree,\n","        use_label_encoder=False,\n","        eval_metric=\"logloss\"\n","    )\n","\n","    score = cross_val_accuracy(model, X_train, y_train)\n","    return score\n","\n","# Run Optuna for XGBoost\n","study_xgboost = optuna.create_study(direction='maximize')\n","study_xgboost.optimize(objective_xgboost, n_trials=50)\n","\n","# Print and save results\n","print(\"XGBoost Results:\")\n","print(\"Best Parameters:\", study_xgboost.best_params)\n","print(\"Best Accuracy:\", study_xgboost.best_value)\n","\n","# Save results\n","xgboost_results = pd.DataFrame([{\n","    'Model': 'XGBoost',\n","    'Best Accuracy': study_xgboost.best_value,\n","    'Best Parameters': study_xgboost.best_params\n","}])\n"],"metadata":{"id":"CEi0KkCAnGX-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#LightGBM\n","\n","from lightgbm import LGBMClassifier\n","\n","def objective_lightgbm(trial):\n","    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n","    max_depth = trial.suggest_int('max_depth', 3, 20)\n","    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n","    num_leaves = trial.suggest_int('num_leaves', 20, 50)\n","\n","    model = LGBMClassifier(\n","        n_estimators=n_estimators,\n","        max_depth=max_depth,\n","        learning_rate=learning_rate,\n","        num_leaves=num_leaves\n","    )\n","\n","    score = cross_val_accuracy(model, X_train, y_train)\n","    return score\n","\n","# Run Optuna for LightGBM\n","study_lightgbm = optuna.create_study(direction='maximize')\n","study_lightgbm.optimize(objective_lightgbm, n_trials=50)\n","\n","# Print and save results\n","print(\"LightGBM Results:\")\n","print(\"Best Parameters:\", study_lightgbm.best_params)\n","print(\"Best Accuracy:\", study_lightgbm.best_value)\n","\n","# Save results\n","lightgbm_results = pd.DataFrame([{\n","    'Model': 'LightGBM',\n","    'Best Accuracy': study_lightgbm.best_value,\n","    'Best Parameters': study_lightgbm.best_params\n","}])\n"],"metadata":{"id":"3DE4jKKknGau"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#CatBoost\n","\n","def objective_catboost(trial):\n","    depth = trial.suggest_int('depth', 3, 12)\n","    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n","    iterations = trial.suggest_int('iterations', 50, 200)\n","    subsample = trial.suggest_uniform('subsample', 0.5, 1.0)\n","    model = CatBoostClassifier(depth=depth, learning_rate=learning_rate,\n","                               iterations=iterations, subsample=subsample, verbose=0)\n","    return cross_val_accuracy(model, X_train, y_train)\n","\n","study_catboost = optuna.create_study(direction='maximize')\n","study_catboost.optimize(objective_catboost, n_trials=50)\n","print(\"CatBoost Results:\", study_catboost.best_params)\n"],"metadata":{"id":"fm57g8gsnGdO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#AdaBoost\n","\n","def objective_adaboost(trial):\n","    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n","    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n","    algorithm = trial.suggest_categorical('algorithm', ['SAMME', 'SAMME.R'])\n","    model = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate, algorithm=algorithm)\n","    return cross_val_accuracy(model, X_train, y_train)\n","\n","study_adaboost = optuna.create_study(direction='maximize')\n","study_adaboost.optimize(objective_adaboost, n_trials=50)\n","print(\"AdaBoost Results:\", study_adaboost.best_params)\n"],"metadata":{"id":"yz-CvE3BnGfu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Neural Network\n","def create_nn(num_units, dropout_rate, learning_rate, input_shape): # Define the create_nn function here\n","    model = Sequential([\n","        Dense(num_units, activation='relu', input_shape=input_shape),\n","        Dropout(dropout_rate),\n","        Dense(num_units, activation='relu'),\n","        Dropout(dropout_rate),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","def objective_nn(trial):\n","    num_units = trial.suggest_int('num_units', 32, 256)\n","    dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.5)\n","    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n","    model = KerasClassifier(build_fn=create_nn, num_units=num_units, # Now create_nn is defined and can be used\n","                            dropout_rate=dropout_rate, learning_rate=learning_rate,\n","                            input_shape=(X_train.shape[1],), epochs=5, batch_size=32, verbose=0)\n","    return cross_val_accuracy(model, X_train, y_train)\n","\n","study_nn = optuna.create_study(direction='maximize')\n","study_nn.optimize(objective_nn, n_trials=50)\n","print(\"Neural Network Results:\", study_nn.best_params)"],"metadata":{"id":"NiLHWctgnGiO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#MLP\n","\n","from sklearn.neural_network import MLPClassifier\n","\n","def objective_mlp(trial):\n","    hidden_layer_sizes = trial.suggest_categorical('hidden_layer_sizes', [(32,), (64,), (128,), (64, 64)])\n","    alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n","    learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-5, 1e-1)\n","    activation = trial.suggest_categorical('activation', ['identity', 'logistic', 'tanh', 'relu'])\n","    model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, alpha=alpha,\n","                          learning_rate_init=learning_rate_init, activation=activation, max_iter=300)\n","    return cross_val_accuracy(model, X_train, y_train)\n","\n","study_mlp = optuna.create_study(direction='maximize')\n","study_mlp.optimize(objective_mlp, n_trials=50)\n","print(\"MLP Results:\", study_mlp.best_params)\n"],"metadata":{"id":"ylAYphb-nGku"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","from catboost import CatBoostClassifier\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from scikeras.wrappers import KerasClassifier\n","\n","# Add the target column to the dataset\n","probability_dataset = pd.DataFrame(y_train, columns=[\"Target\"])\n","\n","# Function to add probabilities\n","def add_probabilities(model, model_name, X_train, probability_dataset):\n","    if hasattr(model, \"predict_proba\"):\n","        probabilities = model.predict_proba(X_train)[:, 1]  # Positive class probabilities\n","    else:\n","        probabilities = model.predict(X_train).flatten()  # For Neural Networks or others\n","    probability_dataset[model_name] = probabilities\n","    return probability_dataset\n","\n","# Train and add probabilities for each algorithm\n","\n","# 1. SVM\n","print(\"Training SVM...\")\n","svm_model = SVC(**grouped_best_params[\"SVM\"], probability=True)\n","svm_model.fit(X_train, y_train)\n","probability_dataset = add_probabilities(svm_model, \"SVM\", X_train, probability_dataset)\n","\n","# 2. Decision Tree\n","print(\"Training Decision Tree...\")\n","decision_tree_model = DecisionTreeClassifier(**grouped_best_params[\"Decision Tree\"])\n","decision_tree_model.fit(X_train, y_train)\n","probability_dataset = add_probabilities(decision_tree_model, \"Decision_Tree\", X_train, probability_dataset)\n","\n","# 3. Random Forest\n","print(\"Training Random Forest...\")\n","random_forest_model = RandomForestClassifier(**grouped_best_params[\"Random Forest\"])\n","random_forest_model.fit(X_train, y_train)\n","probability_dataset = add_probabilities(random_forest_model, \"Random_Forest\", X_train, probability_dataset)\n","\n","# 4. Gradient Boosting\n","print(\"Training Gradient Boosting...\")\n","gradient_boosting_model = GradientBoostingClassifier(**study_gradient_boosting.best_params)\n","gradient_boosting_model.fit(X_train, y_train)\n","probability_dataset = add_probabilities(gradient_boosting_model, \"Gradient_Boosting\", X_train, probability_dataset)\n","\n","# 5. XGBoost\n","print(\"Training XGBoost...\")\n","xgboost_model = XGBClassifier(**study_xgboost.best_params, use_label_encoder=False, eval_metric=\"logloss\")\n","xgboost_model.fit(X_train, y_train)\n","probability_dataset = add_probabilities(xgboost_model, \"XGBoost\", X_train, probability_dataset)\n","\n","# 6. LightGBM\n","print(\"Training LightGBM...\")\n","lightgbm_model = LGBMClassifier(**study_lightgbm.best_params)\n","lightgbm_model.fit(X_train, y_train)\n","probability_dataset = add_probabilities(lightgbm_model, \"LightGBM\", X_train, probability_dataset)\n","\n","# 7. CatBoost\n","print(\"Training CatBoost...\")\n","catboost_model = CatBoostClassifier(**study_catboost.best_params, verbose=0)\n","catboost_model.fit(X_train, y_train)\n","probability_dataset = add_probabilities(catboost_model, \"CatBoost\", X_train, probability_dataset)\n","\n","# 8. AdaBoost\n","print(\"Training AdaBoost...\")\n","adaboost_model = AdaBoostClassifier(**study_adaboost.best_params)\n","adaboost_model.fit(X_train, y_train)\n","probability_dataset = add_probabilities(adaboost_model, \"AdaBoost\", X_train, probability_dataset)\n","\n","# 9. Logistic Regression\n","print(\"Training Logistic Regression...\")\n","logistic_model = LogisticRegression(**grouped_best_params[\"Logistic Regression\"])\n","logistic_model.fit(X_train, y_train)\n","probability_dataset = add_probabilities(logistic_model, \"Logistic_Regression\", X_train, probability_dataset)\n","\n","# 10. k-NN\n","print(\"Training k-NN...\")\n","knn_model = KNeighborsClassifier(**grouped_best_params[\"k-NN\"])\n","knn_model.fit(X_train, y_train)\n","probability_dataset = add_probabilities(knn_model, \"k-NN\", X_train, probability_dataset)\n","\n","# 11. Naive Bayes\n","print(\"Training Naive Bayes...\")\n","naive_bayes_model = GaussianNB()\n","naive_bayes_model.fit(X_train, y_train)\n","probability_dataset = add_probabilities(naive_bayes_model, \"Naive_Bayes\", X_train, probability_dataset)\n","\n","# 12. Neural Network\n","print(\"Training Neural Network...\")\n","nn_model = Sequential([\n","    Dense(study_nn.best_params[\"num_units\"], activation='relu', input_shape=(X_train.shape[1],)),\n","    Dropout(study_nn.best_params[\"dropout_rate\"]),\n","    Dense(study_nn.best_params[\"num_units\"], activation='relu'),\n","    Dropout(study_nn.best_params[\"dropout_rate\"]),\n","    Dense(1, activation='sigmoid')\n","])\n","nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","nn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n","nn_probs = nn_model.predict(X_train).flatten()\n","probability_dataset[\"Neural_Network\"] = nn_probs\n","\n","# 13. MLP\n","print(\"Training MLP...\")\n","mlp_model = MLPClassifier(**study_mlp.best_params, max_iter=300)\n","mlp_model.fit(X_train, y_train)\n","probability_dataset = add_probabilities(mlp_model, \"MLP\", X_train, probability_dataset)\n","\n","# Save the Combined Probability Dataset\n","output_path = \"/content/CTDT_OPTUNA_Dataset.csv\"\n","probability_dataset.to_csv(output_path, index=False)\n","print(f\"Combined probability dataset saved at {output_path}\")\n"],"metadata":{"id":"CJtGJcfCnGnm"},"execution_count":null,"outputs":[]}]}